{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keerthi612004/text-to-hand-gesture/blob/main/text_to_hand_gesture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZEsHoAPkLUu",
        "outputId": "a110e5f1-3c25-4912-942f-837cd1406a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Extract features ONLY for label 'O' and print ready-to-paste rows =====\n",
        "!pip install mediapipe\n",
        "import os, cv2, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import mediapipe as mp\n",
        "\n",
        "# --- Mount drive ---\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIG: adjust if your folder name differs (e.g., 'o' vs 'O') ---\n",
        "LABEL = 'O'\n",
        "LETTER_DIR = f'/content/drive/MyDrive/BDA/Preprocessed_data/{LABEL}'  # your enhanced images by letter\n",
        "SAVE_CSV = '/content/drive/MyDrive/BDA/features_O_only.csv'          # mini CSV for just 'O'\n",
        "\n",
        "os.makedirs(os.path.dirname(SAVE_CSV), exist_ok=True)\n",
        "\n",
        "# --- MediaPipe setups (Hands + Holistic fallback) ---\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_holistic = mp.solutions.holistic\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.30)\n",
        "holistic = mp_holistic.Holistic(static_image_mode=True)\n",
        "\n",
        "# --- Enhancement: gamma + CLAHE(L channel) + bilateral denoise ---\n",
        "def enhance_for_features(img, size=(256,256)):\n",
        "    img = cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
        "    # gamma brighten\n",
        "    gamma = 1.8\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([(i / 255.0) ** invGamma * 255 for i in np.arange(256)]).astype(\"uint8\")\n",
        "    img = cv2.LUT(img, table)\n",
        "    # CLAHE on L channel\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    l = clahe.apply(l)\n",
        "    img = cv2.cvtColor(cv2.merge((l, a, b)), cv2.COLOR_LAB2BGR)\n",
        "    # denoise (edge-preserving)\n",
        "    img = cv2.bilateralFilter(img, 7, 75, 75)\n",
        "    return img\n",
        "\n",
        "# --- Simple skin-based crop as a retry path ---\n",
        "def crop_hand_region(img):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    lower = np.array([0, 20, 70], dtype=np.uint8)   # broad skin-ish range\n",
        "    upper = np.array([20, 255, 255], dtype=np.uint8)\n",
        "    mask1 = cv2.inRange(hsv, lower, upper)\n",
        "    # also try a secondary range to be safe\n",
        "    lower2 = np.array([160, 20, 70], dtype=np.uint8)\n",
        "    upper2 = np.array([179, 255, 255], dtype=np.uint8)\n",
        "    mask = cv2.bitwise_or(mask1, cv2.inRange(hsv, lower2, upper2))\n",
        "    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if cnts:\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        # pad a bit\n",
        "        pad = int(0.05*max(w,h))\n",
        "        x = max(0, x-pad); y = max(y-pad); h = h+2*pad; w = w+2*pad;\n",
        "        # ensure bounds stay within image dimensions\n",
        "        img_h, img_w = img.shape[:2]\n",
        "        y = max(0, y)\n",
        "        x = max(0, x)\n",
        "        h = min(img_h - y, h)\n",
        "        w = min(img_w - x, w)\n",
        "        return img[y:y+h, x:x+w]\n",
        "    return img\n",
        "\n",
        "def extract_landmarks_any(image_bgr):\n",
        "    # 1) try Hands\n",
        "    res = hands.process(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    if res.multi_hand_landmarks:\n",
        "        lm = res.multi_hand_landmarks[0].landmark\n",
        "        return [(p.x, p.y, p.z) for p in lm]\n",
        "    # 2) try Holistic\n",
        "    res2 = holistic.process(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    if res2.left_hand_landmarks:\n",
        "        lm = res2.left_hand_landmarks.landmark\n",
        "        return [(p.x, p.y, p.z) for p in lm]\n",
        "    if res2.right_hand_landmarks:\n",
        "        lm = res2.right_hand_landmarks.landmark\n",
        "        return [(p.x, p.y, p.z) for p in lm]\n",
        "    return None\n",
        "\n",
        "rows, failed = [], []\n",
        "\n",
        "if not os.path.isdir(LETTER_DIR):\n",
        "    raise FileNotFoundError(f\"Folder not found: {LETTER_DIR}\")\n",
        "\n",
        "files = [f for f in os.listdir(LETTER_DIR) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "files.sort()\n",
        "\n",
        "for filename in tqdm(files, desc=f\"Extracting '{LABEL}'\"):\n",
        "    path = os.path.join(LETTER_DIR, filename)\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        failed.append(path); continue\n",
        "\n",
        "    # enhanced\n",
        "    enh = enhance_for_features(img)\n",
        "\n",
        "    # try direct\n",
        "    lms = extract_landmarks_any(enh)\n",
        "\n",
        "    # retry with crop if needed\n",
        "    if lms is None:\n",
        "        cropped = crop_hand_region(enh)\n",
        "        if cropped.size != 0:\n",
        "            lms = extract_landmarks_any(cropped)\n",
        "\n",
        "    if lms:\n",
        "        row = {'label': LABEL, 'file': filename}\n",
        "        # ensure exactly 21 landmarks (mediapipe standard)\n",
        "        if len(lms) >= 21:\n",
        "            lms = lms[:21]\n",
        "        elif len(lms) < 21:\n",
        "            # pad with NaNs to keep schema consistent\n",
        "            lms = lms + [(np.nan, np.nan, np.nan)]*(21 - len(lms))\n",
        "\n",
        "        for i, (x,y,z) in enumerate(lms):\n",
        "            row[f'x{i}'] = x\n",
        "            row[f'y{i}'] = y\n",
        "            row[f'z{i}'] = z\n",
        "        rows.append(row)\n",
        "    else:\n",
        "        failed.append(path)\n",
        "\n",
        "# save mini CSV\n",
        "df_o = pd.DataFrame(rows)\n",
        "df_o.to_csv(SAVE_CSV, index=False)\n",
        "print(f\"\\n✅ Saved {len(df_o)} extracted rows to: {SAVE_CSV}\")\n",
        "print(f\"⚠️ Failed: {len(failed)}\")\n",
        "\n",
        "# ALSO: print ready-to-paste CSV lines (matching your main file schema)\n",
        "if len(df_o) > 0:\n",
        "    # build header once\n",
        "    header = ['label','file'] + [f'{ax}{i}' for i in range(21) for ax in ('x','y','z')]\n",
        "    # ensure column order\n",
        "    df_o = df_o.reindex(columns=header)\n",
        "    print(\"\\n======== COPY BELOW (header) ========\")\n",
        "    print(','.join(header))\n",
        "    print(\"======== COPY BELOW (rows) ==========\")\n",
        "    for _, r in df_o.iterrows():\n",
        "        vals = [str(r[c]) if pd.notna(r[c]) else '' for c in header]\n",
        "        print(','.join(vals))\n",
        "else:\n",
        "    print(\"No successful extractions for 'O'. Try lowering confidence to 0.2 or check image quality.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Iar2Y5K9kpFk",
        "outputId": "e20244df-dca8-43ba-fabc-80859b9d097b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Collecting numpy<2 (from mediapipe)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
            "INFO: pip is looking at multiple versions of opencv-contrib-python to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opencv-contrib-python (from mediapipe)\n",
            "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp312-cp312-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, sounddevice, opencv-contrib-python, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.12.0.88\n",
            "    Uninstalling opencv-contrib-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-contrib-python-4.12.0.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 opencv-contrib-python-4.11.0.86 protobuf-4.25.8 sounddevice-0.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2",
                  "google",
                  "numpy"
                ]
              },
              "id": "b27b3ddcb65b4d72b877d4299255d1b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3023783282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ===== Extract features ONLY for label 'O' and print ready-to-paste rows =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install mediapipe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9b6b3cc3",
        "outputId": "be9da459-c68f-42fb-c84d-a1002d0089fa"
      },
      "source": [
        "!pip uninstall pandas -y\n",
        "!pip install pandas mediapipe\n",
        "\n",
        "# Re-import the libraries after re-installation\n",
        "import os, cv2, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import mediapipe as mp\n",
        "\n",
        "# --- Mount drive ---\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIG: adjust if your folder name differs (e.g., 'o' vs 'O') ---\n",
        "LABEL = 'O'\n",
        "LETTER_DIR = f'/content/drive/MyDrive/BDA/Preprocessed_data/{LABEL}'  # your enhanced images by letter\n",
        "SAVE_CSV = '/content/drive/MyDrive/BDA/features_O_only.csv'          # mini CSV for just 'O'\n",
        "\n",
        "os.makedirs(os.path.dirname(SAVE_CSV), exist_ok=True)\n",
        "\n",
        "# --- MediaPipe setups (Hands + Holistic fallback) ---\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_holistic = mp.solutions.holistic\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.30)\n",
        "holistic = mp_holistic.Holistic(static_image_mode=True)\n",
        "\n",
        "# --- Enhancement: gamma + CLAHE(L channel) + bilateral denoise ---\n",
        "def enhance_for_features(img, size=(256,256)):\n",
        "    img = cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
        "    # gamma brighten\n",
        "    gamma = 1.8\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([(i / 255.0) ** invGamma * 255 for i in np.arange(256)]).astype(\"uint8\")\n",
        "    img = cv2.LUT(img, table)\n",
        "    # CLAHE on L channel\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    l = clahe.apply(l)\n",
        "    img = cv2.cvtColor(cv2.merge((l, a, b)), cv2.COLOR_LAB2BGR)\n",
        "    # denoise (edge-preserving)\n",
        "    img = cv2.bilateralFilter(img, 7, 75, 75)\n",
        "    return img\n",
        "\n",
        "# --- Simple skin-based crop as a retry path ---\n",
        "def crop_hand_region(img):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    lower = np.array([0, 20, 70], dtype=np.uint8)   # broad skin-ish range\n",
        "    upper = np.array([20, 255, 255], dtype=np.uint8)\n",
        "    mask1 = cv2.inRange(hsv, lower, upper)\n",
        "    # also try a secondary range to be safe\n",
        "    lower2 = np.array([160, 20, 70], dtype=np.uint8)\n",
        "    upper2 = np.array([179, 255, 255], dtype=np.uint8)\n",
        "    mask = cv2.bitwise_or(mask1, cv2.inRange(hsv, lower2, upper2))\n",
        "    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    if cnts:\n",
        "        c = max(cnts, key=cv2.contourArea)\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        # pad a bit\n",
        "        pad = int(0.05*max(w,h))\n",
        "        x = max(0, x-pad)\n",
        "        y = max(0, y-pad)\n",
        "        h = h+2*pad\n",
        "        w = w+2*pad\n",
        "        # ensure bounds stay within image dimensions\n",
        "        img_h, img_w = img.shape[:2]\n",
        "        y = max(0, y)\n",
        "        x = max(0, x)\n",
        "        h = min(img_h - y, h)\n",
        "        w = min(img_w - x, w)\n",
        "        return img[y:y+h, x:x+w]\n",
        "    return img\n",
        "\n",
        "\n",
        "def extract_landmarks_any(image_bgr):\n",
        "    # 1) try Hands\n",
        "    res = hands.process(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    if res.multi_hand_landmarks:\n",
        "        lm = res.multi_hand_landmarks[0].landmark\n",
        "        return [(p.x, p.y, p.z) for p in lm]\n",
        "    # 2) try Holistic\n",
        "    res2 = holistic.process(cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB))\n",
        "    if res2.left_hand_landmarks:\n",
        "        lm = res2.left_hand_landmarks.landmark\n",
        "        return [(p.x, p.y, p.z) for p in lm]\n",
        "    if res2.right_hand_landmarks:\n",
        "        lm = res2.right_hand_landmarks.landmark\n",
        "        return [(p.x, p.y, p.z) for p in lm]\n",
        "    return None\n",
        "\n",
        "rows, failed = [], []\n",
        "\n",
        "if not os.path.isdir(LETTER_DIR):\n",
        "    raise FileNotFoundError(f\"Folder not found: {LETTER_DIR}\")\n",
        "\n",
        "files = [f for f in os.listdir(LETTER_DIR) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "files.sort()\n",
        "\n",
        "for filename in tqdm(files, desc=f\"Extracting '{LABEL}'\"):\n",
        "    path = os.path.join(LETTER_DIR, filename)\n",
        "    img = cv2.imread(path)\n",
        "    if img is None:\n",
        "        failed.append(path); continue\n",
        "\n",
        "    # enhanced\n",
        "    enh = enhance_for_features(img)\n",
        "\n",
        "    # try direct\n",
        "    lms = extract_landmarks_any(enh)\n",
        "\n",
        "    # retry with crop if needed\n",
        "    if lms is None:\n",
        "        cropped = crop_hand_region(enh)\n",
        "        if cropped.size != 0:\n",
        "            lms = extract_landmarks_any(cropped)\n",
        "\n",
        "    if lms:\n",
        "        row = {'label': LABEL, 'file': filename}\n",
        "        # ensure exactly 21 landmarks (mediapipe standard)\n",
        "        if len(lms) >= 21:\n",
        "            lms = lms[:21]\n",
        "        elif len(lms) < 21:\n",
        "            # pad with NaNs to keep schema consistent\n",
        "            lms = lms + [(np.nan, np.nan, np.nan)]*(21 - len(lms))\n",
        "\n",
        "        for i, (x,y,z) in enumerate(lms):\n",
        "            row[f'x{i}'] = x\n",
        "            row[f'y{i}'] = y\n",
        "            row[f'z{i}'] = z\n",
        "        rows.append(row)\n",
        "    else:\n",
        "        failed.append(path)\n",
        "\n",
        "# save mini CSV\n",
        "df_o = pd.DataFrame(rows)\n",
        "df_o.to_csv(SAVE_CSV, index=False)\n",
        "print(f\"\\n✅ Saved {len(df_o)} extracted rows to: {SAVE_CSV}\")\n",
        "print(f\"⚠️ Failed: {len(failed)}\")\n",
        "\n",
        "# ALSO: print ready-to-paste CSV lines (matching your main file schema)\n",
        "if len(df_o) > 0:\n",
        "    # build header once\n",
        "    header = ['label','file'] + [f'{ax}{i}' for i in range(21) for ax in ('x','y','z')]\n",
        "    # ensure column order\n",
        "    df_o = df_o.reindex(columns=header)\n",
        "    print(\"\\n======== COPY BELOW (header) ========\")\n",
        "    print(','.join(header))\n",
        "    print(\"======== COPY BELOW (rows) ==========\")\n",
        "    for _, r in df_o.iterrows():\n",
        "        vals = [str(r[c]) if pd.notna(r[c]) else '' for c in header]\n",
        "        print(','.join(vals))\n",
        "else:\n",
        "    print(\"No successful extractions for 'O'. Try lowering confidence to 0.2 or check image quality.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pandas 2.3.3\n",
            "Uninstalling pandas-2.3.3:\n",
            "  Successfully uninstalled pandas-2.3.3\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.21)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
            "Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "Installing collected packages: pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "f8e0af7bf52f4e39ba11970db719781e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting 'O': 100%|██████████| 300/300 [00:39<00:00,  7.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Saved 4 extracted rows to: /content/drive/MyDrive/BDA/features_O_only.csv\n",
            "⚠️ Failed: 296\n",
            "\n",
            "======== COPY BELOW (header) ========\n",
            "label,file,x0,y0,z0,x1,y1,z1,x2,y2,z2,x3,y3,z3,x4,y4,z4,x5,y5,z5,x6,y6,z6,x7,y7,z7,x8,y8,z8,x9,y9,z9,x10,y10,z10,x11,y11,z11,x12,y12,z12,x13,y13,z13,x14,y14,z14,x15,y15,z15,x16,y16,z16,x17,y17,z17,x18,y18,z18,x19,y19,z19,x20,y20,z20\n",
            "======== COPY BELOW (rows) ==========\n",
            "O,1200.jpg,0.5698581337928772,0.18805618584156036,2.620690793264657e-07,0.5920262336730957,0.18876826763153076,-0.004561762325465679,0.6234095692634583,0.18219773471355438,-0.008262060582637787,0.6462398171424866,0.1793142706155777,-0.012676806189119816,0.6644434332847595,0.1764194816350937,-0.016964832320809364,0.6281495094299316,0.15269194543361664,-0.004288057330995798,0.6645672917366028,0.16203689575195312,-0.014773876406252384,0.6894688010215759,0.17363449931144714,-0.023177409544587135,0.7060701847076416,0.18521180748939514,-0.028140660375356674,0.6258429288864136,0.15314032137393951,-0.007064793258905411,0.6629476547241211,0.16287508606910706,-0.016271941363811493,0.6865783929824829,0.17536868155002594,-0.021911220625042915,0.7025043368339539,0.18730145692825317,-0.025368770584464073,0.623618483543396,0.15802259743213654,-0.01088855043053627,0.6601020693778992,0.16723431646823883,-0.01989128813147545,0.6825971603393555,0.17874611914157867,-0.021573323756456375,0.6974396705627441,0.18931585550308228,-0.021294748410582542,0.621699333190918,0.1675357222557068,-0.015475903637707233,0.6539891958236694,0.1733362227678299,-0.02153184451162815,0.6734379529953003,0.18183791637420654,-0.021337710320949554,0.686879575252533,0.19067278504371643,-0.019995639100670815\n",
            "O,1476.jpg,0.6330793499946594,0.25129935145378113,-4.487476985559624e-07,0.587544322013855,0.2584545314311981,-0.004695810843259096,0.5454984307289124,0.25688818097114563,-0.012256097048521042,0.5153650045394897,0.25884222984313965,-0.02109721302986145,0.49185171723365784,0.26491037011146545,-0.031023260205984116,0.5331815481185913,0.20953842997550964,-0.009394874796271324,0.5015445947647095,0.23773956298828125,-0.029073672369122505,0.4972798228263855,0.27256929874420166,-0.04579490050673485,0.5004444122314453,0.2970598340034485,-0.054786745458841324,0.5508290529251099,0.20546187460422516,-0.0147402910515666,0.519893229007721,0.24436518549919128,-0.02997225522994995,0.5120531320571899,0.2824466824531555,-0.04002910107374191,0.5124602317810059,0.3095991611480713,-0.0459241084754467,0.5708465576171875,0.20773407816886902,-0.021229276433587074,0.540001392364502,0.24691195785999298,-0.034315261989831924,0.5324134826660156,0.28144383430480957,-0.036743443459272385,0.5330502986907959,0.3051780164241791,-0.03696942701935768,0.593072235584259,0.2132682353258133,-0.02810647524893284,0.565708577632904,0.24439489841461182,-0.038786470890045166,0.5563910603523254,0.270682692527771,-0.03730585798621178,0.5528367757797241,0.289591908454895,-0.03372984007000923\n",
            "O,897.jpg,0.42255914211273193,0.5282791256904602,-2.701991093090328e-07,0.39502209424972534,0.5243850946426392,0.007329532876610756,0.36908653378486633,0.5108081102371216,0.00786987878382206,0.3615877032279968,0.49550554156303406,0.004715851042419672,0.35859090089797974,0.4847523868083954,0.0023482891265302896,0.34857481718063354,0.4875485301017761,0.01331072673201561,0.3449659049510956,0.4720156788825989,0.007047871593385935,0.358912855386734,0.4776207208633423,-0.0007744018803350627,0.3678063750267029,0.486937552690506,-0.005797433201223612,0.3549956679344177,0.4767993688583374,0.005158616229891777,0.3553038239479065,0.46408751606941223,0.0021972698159515858,0.36770254373550415,0.4708423614501953,-0.002328434493392706,0.3750782608985901,0.4789391756057739,-0.005766605958342552,0.3655030429363251,0.4669666886329651,-0.003434455255046487,0.36531758308410645,0.456202894449234,-0.0038483005482703447,0.3760227859020233,0.4635154902935028,-0.0020379337947815657,0.382205069065094,0.47077420353889465,-0.0015327638247981668,0.3786160349845886,0.4580322206020355,-0.012270255945622921,0.3787292540073395,0.44812658429145813,-0.009134995751082897,0.386851042509079,0.4556388556957245,-0.003292615292593837,0.39253541827201843,0.46201083064079285,0.0007550811278633773\n",
            "O,898.jpg,0.4160744547843933,0.5246407985687256,-1.938494023079329e-07,0.38747841119766235,0.5255303978919983,0.004821293987333775,0.36338698863983154,0.5106838345527649,0.00432504341006279,0.35753950476646423,0.4931354522705078,0.0009599134791642427,0.3597468137741089,0.47919902205467224,-0.0016304076416417956,0.3470907509326935,0.4886297285556793,0.00928746908903122,0.34239885210990906,0.47258704900741577,0.0026868905406445265,0.3539685904979706,0.4787423610687256,-0.004424198996275663,0.362236350774765,0.4880940020084381,-0.008654081262648106,0.35777050256729126,0.4763934016227722,0.0020528428722172976,0.35632169246673584,0.4650209844112396,-0.0018629017286002636,0.3660680055618286,0.47168421745300293,-0.006239023059606552,0.374318927526474,0.4800388514995575,-0.008932881988584995,0.3720036447048187,0.46587181091308594,-0.0056793419644236565,0.37134379148483276,0.4568632245063782,-0.007771276403218508,0.3785856068134308,0.4657198488712311,-0.006382966414093971,0.38424646854400635,0.47468870878219604,-0.005401175934821367,0.3880556523799896,0.45748674869537354,-0.013683555647730827,0.38726505637168884,0.4503634572029114,-0.01266277115792036,0.39273974299430847,0.4598482847213745,-0.00792944896966219,0.3969828188419342,0.46749210357666016,-0.0041112168692052364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68f4c8ab",
        "outputId": "f7c27c89-cfed-4e9b-d7a4-bda77f57eabf"
      },
      "source": [
        "import os\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG PATHS\n",
        "# ----------------------------\n",
        "GEN_FRAMES_PATH = \"/content/drive/MyDrive/BDA/generated_frames\"\n",
        "REAL_FRAMES_PATH = \"/content/drive/MyDrive/BDA/real_signs\"\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(GEN_FRAMES_PATH, exist_ok=True)\n",
        "os.makedirs(REAL_FRAMES_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Directories created or already exist: {GEN_FRAMES_PATH} and {REAL_FRAMES_PATH}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories created or already exist: /content/drive/MyDrive/BDA/generated_frames and /content/drive/MyDrive/BDA/real_signs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI based text to sign generation\n"
      ],
      "metadata": {
        "id": "dN6bAIX5slnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aKnN5ZRxoFeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os, cv2, numpy as np, pandas as pd\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "import tempfile\n",
        "\n",
        "# ---- CONFIG (adjust to your Drive paths) ----\n",
        "DATA_PATH = \"/content/drive/MyDrive/BDA/Stratified_data\"\n",
        "FEATURES_CSV = \"/content/drive/MyDrive/BDA/feature_extraction/features_.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/BDA\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load once\n",
        "df = pd.read_csv(FEATURES_CSV)\n",
        "\n",
        "# Font fallback\n",
        "def get_font():\n",
        "    try:\n",
        "        return ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\", 50)\n",
        "    except:\n",
        "        return ImageFont.load_default()\n",
        "\n",
        "FONT = get_font()\n",
        "\n",
        "def generate_animation(word, seconds_per_letter):\n",
        "    if not word or not word.strip():\n",
        "        return None, \"Please type some text.\"\n",
        "\n",
        "    word_upper = word.strip().upper()\n",
        "    unique_letters = list(set(word_upper))\n",
        "    subset = df[df[\"label\"].isin(unique_letters)].copy()\n",
        "\n",
        "    frames = []\n",
        "    paths = []\n",
        "    last_valid = None\n",
        "\n",
        "    # Build sequence of image paths\n",
        "    for ch in word_upper:\n",
        "        row = subset[subset[\"label\"] == ch]\n",
        "        if not row.empty:\n",
        "            sel = row.sample(1, random_state=42).iloc[0]\n",
        "            p = os.path.join(DATA_PATH, ch, sel[\"file\"])\n",
        "            paths.append(p)\n",
        "            last_valid = p\n",
        "        else:\n",
        "            if last_valid:\n",
        "                paths.append(last_valid)\n",
        "\n",
        "    if not paths:\n",
        "        return None, \"No matching letters found in dataset.\"\n",
        "\n",
        "    # Create labeled frames + transitions\n",
        "    for i, path in enumerate(paths):\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            continue\n",
        "        img = cv2.resize(img, (400, 400))\n",
        "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        pil = Image.fromarray(rgb)\n",
        "        draw = ImageDraw.Draw(pil)\n",
        "        draw.text((20, 20), os.path.basename(os.path.dirname(path)), fill=(255, 255, 255), font=FONT)\n",
        "        frames.append(np.array(pil))\n",
        "\n",
        "        # Fade to next\n",
        "        if i < len(paths)-1:\n",
        "            nxt = cv2.imread(paths[i+1])\n",
        "            if nxt is not None:\n",
        "                nxt = cv2.resize(nxt, (400, 400))\n",
        "                nxt_rgb = cv2.cvtColor(nxt, cv2.COLOR_BGR2RGB)\n",
        "                for a in np.linspace(0, 1, 10):\n",
        "                    blend = cv2.addWeighted(rgb, 1-a, nxt_rgb, a, 0)\n",
        "                    frames.append(blend)\n",
        "\n",
        "    if not frames:\n",
        "        return None, \"Failed to build frames.\"\n",
        "\n",
        "    # FPS so that each letter ≈ seconds_per_letter\n",
        "    # Each letter adds 1 base frame + ~10 transition frames → ≈11 frames/letter\n",
        "    frames_per_letter = 11\n",
        "    total_letters = len(paths)\n",
        "    total_frames = len(frames)\n",
        "    # Target fps = total_frames / (total_letters * seconds_per_letter)\n",
        "    fps = max(1, int(round(total_frames / (total_letters * seconds_per_letter))))\n",
        "\n",
        "    # Save mp4 to a temp file and also copy to Drive\n",
        "    tmpfile = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False)\n",
        "    tmpfile.close()\n",
        "    clip = ImageSequenceClip(frames, fps=fps)\n",
        "    clip.write_videofile(tmpfile.name, codec=\"libx264\", fps=fps, audio=False, verbose=False, logger=None)\n",
        "\n",
        "    # Also save a named copy under your Drive output\n",
        "    final_path = os.path.join(OUTPUT_DIR, f\"{word.lower()}_animation.mp4\")\n",
        "    clip.write_videofile(final_path, codec=\"libx264\", fps=fps, audio=False, verbose=False, logger=None)\n",
        "\n",
        "    return tmpfile.name, f\"Saved to Drive: {final_path}\"\n",
        "\n",
        "with gr.Blocks(title=\"Text → Sign Animation\") as demo:\n",
        "    gr.Markdown(\"## 🤟 Text-to-Sign Gesture Animator (UI-only)\\nType text below to generate a labeled sign animation.\")\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(label=\"Enter text\", value=\"HELLO\")\n",
        "        secs = gr.Slider(label=\"Duration per letter (seconds)\", minimum=2, maximum=5, value=3, step=1)\n",
        "    btn = gr.Button(\"🎬 Generate Animation\")\n",
        "    out_video = gr.Video(label=\"Preview\")\n",
        "    out_msg = gr.Markdown()\n",
        "\n",
        "    btn.click(fn=generate_animation, inputs=[inp, secs], outputs=[out_video, out_msg])\n",
        "\n",
        "demo.launch(debug=False)  # in Colab it opens right below this cell\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "egoTU031quNE",
        "outputId": "1cd7c7ed-c0d0-493e-ede5-b8041c98cca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b891d4169ee8029ebb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b891d4169ee8029ebb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# METRIC EVALUATION FOR SIGN ANIMATION GENERATION\n",
        "# ======================================================\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import lpips\n",
        "from scipy import linalg\n",
        "from tqdm import tqdm\n",
        "import clip\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG PATHS\n",
        "# ----------------------------\n",
        "GEN_FRAMES_PATH = \"/content/drive/MyDrive/BDA/generated_frames\"\n",
        "REAL_FRAMES_PATH = \"/content/drive/MyDrive/BDA/real_signs\"\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(GEN_FRAMES_PATH, exist_ok=True)\n",
        "os.makedirs(REAL_FRAMES_PATH, exist_ok=True)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------------------\n",
        "# 1️⃣ SSIM - Structural Similarity\n",
        "# ----------------------------\n",
        "def compute_ssim(gen_path):\n",
        "    images = sorted([os.path.join(gen_path, f) for f in os.listdir(gen_path) if f.endswith(\".png\") or f.endswith(\".jpg\")])\n",
        "    ssim_values = []\n",
        "    for i in range(len(images) - 1):\n",
        "        img1 = np.array(Image.open(images[i]).convert(\"L\"))\n",
        "        img2 = np.array(Image.open(images[i+1]).convert(\"L\"))\n",
        "        ssim_values.append(ssim(img1, img2))\n",
        "    return np.mean(ssim_values)\n",
        "\n",
        "# ----------------------------\n",
        "# 2️⃣ LPIPS - Perceptual Similarity\n",
        "# ----------------------------\n",
        "def compute_lpips(real_path, gen_path):\n",
        "    model = lpips.LPIPS(net='alex').to(device)\n",
        "    real_imgs = sorted([os.path.join(real_path, f) for f in os.listdir(real_path)])\n",
        "    gen_imgs = sorted([os.path.join(gen_path, f) for f in os.listdir(gen_path)])\n",
        "    scores = []\n",
        "    # Ensure both lists have the same length\n",
        "    min_len = min(len(real_imgs), len(gen_imgs))\n",
        "    for r, g in zip(real_imgs[:min_len], gen_imgs[:min_len]):\n",
        "        r_img = lpips.im2tensor(lpips.load_image(r)).to(device)\n",
        "        g_img = lpips.im2tensor(lpips.load_image(g)).to(device)\n",
        "        with torch.no_grad():\n",
        "            d = model(r_img, g_img)\n",
        "        scores.append(d.item())\n",
        "    return np.mean(scores)\n",
        "\n",
        "# ----------------------------\n",
        "# 3️⃣ FID - Fréchet Inception Distance\n",
        "# ----------------------------\n",
        "def compute_fid(real_path, gen_path):\n",
        "    inception = models.inception_v3(pretrained=True, transform_input=False).to(device).eval()\n",
        "    preprocess = T.Compose([\n",
        "        T.Resize((299, 299)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "    ])\n",
        "\n",
        "    def get_feats(folder):\n",
        "        feats = []\n",
        "        for img_name in tqdm(os.listdir(folder), desc=f\"Extracting features from {folder}\"):\n",
        "            path = os.path.join(folder, img_name)\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            x = preprocess(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                # Use the auxiliary logits for FID\n",
        "                f = inception(x)[0] if isinstance(inception(x), tuple) else inception(x)\n",
        "            feats.append(f.cpu().numpy().flatten())\n",
        "        return np.array(feats)\n",
        "\n",
        "    real_feats = get_feats(real_path)\n",
        "    gen_feats = get_feats(gen_path)\n",
        "\n",
        "    mu1, sigma1 = real_feats.mean(axis=0), np.cov(real_feats, rowvar=False)\n",
        "    mu2, sigma2 = gen_feats.mean(axis=0), np.cov(gen_feats, rowvar=False)\n",
        "    # Ensure square root of a matrix is real\n",
        "    covmean = linalg.sqrtm(sigma1.dot(sigma2), disp=False)[0]\n",
        "    if not np.isfinite(covmean).all():\n",
        "        print(\"FID calculation failed: sqrtm returned non-finite values.\")\n",
        "        return np.nan # Or raise an error\n",
        "\n",
        "    fid = np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "    return np.real(fid)\n",
        "\n",
        "# ----------------------------\n",
        "# 4️⃣ CLIP Semantic Similarity\n",
        "# ----------------------------\n",
        "def compute_clip_similarity(text, gen_path):\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    text_tokens = clip.tokenize([text]).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "\n",
        "    img_paths = sorted([os.path.join(gen_path, f) for f in os.listdir(gen_path)])\n",
        "    sims = []\n",
        "    for path in tqdm(img_paths, desc=\"Evaluating CLIP similarity\"):\n",
        "        img = preprocess(Image.open(path)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            img_feat = model.encode_image(img)\n",
        "        sims.append(torch.cosine_similarity(img_feat, text_features).item())\n",
        "    return np.mean(sims)\n",
        "\n",
        "# ----------------------------\n",
        "# RUN ALL METRICS\n",
        "# ----------------------------\n",
        "text_input = \"HELLO\"  # example phrase to evaluate\n",
        "ssim_score = compute_ssim(GEN_FRAMES_PATH)\n",
        "lpips_score = compute_lpips(REAL_FRAMES_PATH, GEN_FRAMES_PATH)\n",
        "fid_score = compute_fid(REAL_FRAMES_PATH, GEN_FRAMES_PATH)\n",
        "clip_score = compute_clip_similarity(text_input, GEN_FRAMES_PATH)\n",
        "\n",
        "print(\"\\n===== SIGN ANIMATION QUALITY METRICS =====\")\n",
        "print(f\"🖼️ Structural Similarity (SSIM): {ssim_score:.3f}  → Higher = smoother transitions\")\n",
        "print(f\"👁️ Perceptual Distance (LPIPS): {lpips_score:.3f}  → Lower = more realistic visuals\")\n",
        "print(f\"📈 Fréchet Inception Distance (FID): {fid_score:.2f}  → Lower = closer to real distribution\")\n",
        "print(f\"🔤 CLIP Semantic Similarity: {clip_score:.3f}  → Higher = semantically aligned\")"
      ],
      "metadata": {
        "id": "yZGUDxfyrkRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1e02d3-6d03-44ec-b0cc-c7764319fd0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features from /content/drive/MyDrive/BDA/real_signs: 0it [00:00, ?it/s]\n",
            "Extracting features from /content/drive/MyDrive/BDA/generated_frames: 0it [00:00, ?it/s]\n",
            "WARNING:py.warnings:/tmp/ipython-input-1437785339.py:85: RuntimeWarning: Mean of empty slice.\n",
            "  mu1, sigma1 = real_feats.mean(axis=0), np.cov(real_feats, rowvar=False)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-1437785339.py:85: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  mu1, sigma1 = real_feats.mean(axis=0), np.cov(real_feats, rowvar=False)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-1437785339.py:86: RuntimeWarning: Mean of empty slice.\n",
            "  mu2, sigma2 = gen_feats.mean(axis=0), np.cov(gen_feats, rowvar=False)\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-1437785339.py:86: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  mu2, sigma2 = gen_feats.mean(axis=0), np.cov(gen_feats, rowvar=False)\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-1437785339.py:88: DeprecationWarning: The `disp` argument is deprecated and will be removed in SciPy 1.18.0.\n",
            "  covmean = linalg.sqrtm(sigma1.dot(sigma2), disp=False)[0]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FID calculation failed: sqrtm returned non-finite values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating CLIP similarity: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SIGN ANIMATION QUALITY METRICS =====\n",
            "🖼️ Structural Similarity (SSIM): nan  → Higher = smoother transitions\n",
            "👁️ Perceptual Distance (LPIPS): nan  → Lower = more realistic visuals\n",
            "📈 Fréchet Inception Distance (FID): nan  → Lower = closer to real distribution\n",
            "🔤 CLIP Semantic Similarity: nan  → Higher = semantically aligned\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# METRIC EVALUATION FOR SIGN ANIMATION GENERATION\n",
        "# ======================================================\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import lpips\n",
        "from scipy import linalg\n",
        "from tqdm import tqdm\n",
        "import clip\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG PATHS\n",
        "# ----------------------------\n",
        "GEN_FRAMES_PATH = \"/content/drive/MyDrive/BDA/generated_frames\"\n",
        "REAL_FRAMES_PATH = \"/content/drive/MyDrive/BDA/real_signs\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ----------------------------\n",
        "# 1️⃣ SSIM - Structural Similarity\n",
        "# ----------------------------\n",
        "def compute_ssim(gen_path):\n",
        "    images = sorted([os.path.join(gen_path, f) for f in os.listdir(gen_path) if f.endswith(\".png\") or f.endswith(\".jpg\")])\n",
        "    ssim_values = []\n",
        "    for i in range(len(images) - 1):\n",
        "        img1 = np.array(Image.open(images[i]).convert(\"L\"))\n",
        "        img2 = np.array(Image.open(images[i+1]).convert(\"L\"))\n",
        "        ssim_values.append(ssim(img1, img2))\n",
        "    return np.mean(ssim_values)\n",
        "\n",
        "# ----------------------------\n",
        "# 2️⃣ LPIPS - Perceptual Similarity\n",
        "# ----------------------------\n",
        "def compute_lpips(real_path, gen_path):\n",
        "    model = lpips.LPIPS(net='alex').to(device)\n",
        "    real_imgs = sorted([os.path.join(real_path, f) for f in os.listdir(real_path)])\n",
        "    gen_imgs = sorted([os.path.join(gen_path, f) for f in os.listdir(gen_path)])\n",
        "    scores = []\n",
        "    for r, g in zip(real_imgs, gen_imgs):\n",
        "        r_img = lpips.im2tensor(lpips.load_image(r)).to(device)\n",
        "        g_img = lpips.im2tensor(lpips.load_image(g)).to(device)\n",
        "        with torch.no_grad():\n",
        "            d = model(r_img, g_img)\n",
        "        scores.append(d.item())\n",
        "    return np.mean(scores)\n",
        "\n",
        "# ----------------------------\n",
        "# 3️⃣ FID - Fréchet Inception Distance\n",
        "# ----------------------------\n",
        "def compute_fid(real_path, gen_path):\n",
        "    inception = models.inception_v3(pretrained=True, transform_input=False).to(device).eval()\n",
        "    preprocess = T.Compose([\n",
        "        T.Resize((299, 299)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "    ])\n",
        "\n",
        "    def get_feats(folder):\n",
        "        feats = []\n",
        "        for img_name in tqdm(os.listdir(folder), desc=f\"Extracting features from {folder}\"):\n",
        "            path = os.path.join(folder, img_name)\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            x = preprocess(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                f = inception(x)\n",
        "            feats.append(f.cpu().numpy().flatten())\n",
        "        return np.array(feats)\n",
        "\n",
        "    real_feats = get_feats(real_path)\n",
        "    gen_feats = get_feats(gen_path)\n",
        "\n",
        "    mu1, sigma1 = real_feats.mean(axis=0), np.cov(real_feats, rowvar=False)\n",
        "    mu2, sigma2 = gen_feats.mean(axis=0), np.cov(gen_feats, rowvar=False)\n",
        "    fid = np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * linalg.sqrtm(sigma1.dot(sigma2)))\n",
        "    return np.real(fid)\n",
        "\n",
        "# ----------------------------\n",
        "# 4️⃣ CLIP Semantic Similarity\n",
        "# ----------------------------\n",
        "def compute_clip_similarity(text, gen_path):\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    text_tokens = clip.tokenize([text]).to(device)\n",
        "    text_features = model.encode_text(text_tokens)\n",
        "\n",
        "    img_paths = sorted([os.path.join(gen_path, f) for f in os.listdir(gen_path)])\n",
        "    sims = []\n",
        "    for path in tqdm(img_paths, desc=\"Evaluating CLIP similarity\"):\n",
        "        img = preprocess(Image.open(path)).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            img_feat = model.encode_image(img)\n",
        "        sims.append(torch.cosine_similarity(img_feat, text_features).item())\n",
        "    return np.mean(sims)\n",
        "\n",
        "# ----------------------------\n",
        "# RUN ALL METRICS\n",
        "# ----------------------------\n",
        "text_input = \"HELLO\"  # example phrase to evaluate\n",
        "ssim_score = compute_ssim(GEN_FRAMES_PATH)\n",
        "lpips_score = compute_lpips(REAL_FRAMES_PATH, GEN_FRAMES_PATH)\n",
        "fid_score = compute_fid(REAL_FRAMES_PATH, GEN_FRAMES_PATH)\n",
        "clip_score = compute_clip_similarity(text_input, GEN_FRAMES_PATH)\n",
        "\n",
        "print(\"\\n===== SIGN ANIMATION QUALITY METRICS =====\")\n",
        "print(f\"🖼 Structural Similarity (SSIM): {ssim_score:.3f}  → Higher = smoother transitions\")\n",
        "print(f\"👁 Perceptual Distance (LPIPS): {lpips_score:.3f}  → Lower = more realistic visuals\")\n",
        "print(f\"📈 Fréchet Inception Distance (FID): {fid_score:.2f}  → Lower = closer to real distribution\")\n",
        "print(f\"🔤 CLIP Semantic Similarity: {clip_score:.3f}  → Higher = semantically aligned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIMuSJFOOzrD",
        "outputId": "465c8892-3881-4679-e788-58b84b372c31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features from /content/drive/MyDrive/BDA/real_signs: 0it [00:00, ?it/s]\n",
            "Extracting features from /content/drive/MyDrive/BDA/generated_frames: 0it [00:00, ?it/s]\n",
            "WARNING:py.warnings:/tmp/ipython-input-1844415553.py:77: RuntimeWarning: Mean of empty slice.\n",
            "  mu1, sigma1 = real_feats.mean(axis=0), np.cov(real_feats, rowvar=False)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis, **keepdims_kw)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-1844415553.py:77: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  mu1, sigma1 = real_feats.mean(axis=0), np.cov(real_feats, rowvar=False)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
            "  c *= np.true_divide(1, fact)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
            "  c *= np.true_divide(1, fact)\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-1844415553.py:78: RuntimeWarning: Mean of empty slice.\n",
            "  mu2, sigma2 = gen_feats.mean(axis=0), np.cov(gen_feats, rowvar=False)\n",
            "\n",
            "WARNING:py.warnings:/tmp/ipython-input-1844415553.py:78: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  mu2, sigma2 = gen_feats.mean(axis=0), np.cov(gen_feats, rowvar=False)\n",
            "\n",
            "Evaluating CLIP similarity: 0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SIGN ANIMATION QUALITY METRICS =====\n",
            "🖼 Structural Similarity (SSIM): nan  → Higher = smoother transitions\n",
            "👁 Perceptual Distance (LPIPS): nan  → Lower = more realistic visuals\n",
            "📈 Fréchet Inception Distance (FID): nan  → Lower = closer to real distribution\n",
            "🔤 CLIP Semantic Similarity: nan  → Higher = semantically aligned\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ac92840",
        "outputId": "f6b5b6df-90d5-4183-839a-7bf22071fe3b"
      },
      "source": [
        "!pip install ftfy regex tqdm git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-dslz1egm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-dslz1egm\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=60779c3d79967a6a811cbcee2554700ed7b6e2b0d01dd21e2533841cc82ebf1b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ghtpf_wn/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75845588",
        "outputId": "92a477ff-b1f5-4736-c69e-cf0e0c2fb065"
      },
      "source": [
        "!pip install lpips"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.2)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.2.1->lpips) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.3)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    }
  ]
}